import os
import logging
import json
import hashlib
import asyncio
import time  # <-- ADDED FOR RETRY DELAY
from google.api_core.exceptions import ResourceExhausted  # <-- ADDED FOR 429 ERROR
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from sentence_transformers import CrossEncoder
from config import VECTOR_STORE_DIR, SCIENTIFIC_GENAI_API_KEY
import PIL.Image
from enhanced_question_deduplication import deduplicator

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Constants for the fallback messages
FALLBACK_NO_INFO = "ÙŠØ§ Ø²ÙˆØ²ØŒ Ø³Ø¤Ø§Ù„Ùƒ ÙƒÙˆÙŠØ³ Ø¬Ø¯Ø§ Ø¨Ø³ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± ÙÙŠ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù„ÙŠ Ù…Ø¹Ø§ÙŠØ§. Ù…Ù…ÙƒÙ† ØªØ³Ø£Ù„ Ø¯ÙƒØªÙˆØ± Ù†Ø§ØµØ± ÙÙŠ Ø§Ù„Ù€ mini-group Ø¹Ø´Ø§Ù† ØªØ§Ø®Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ø¯Ù‚ÙŠÙ‚Ø©."
FALLBACK_ERROR = "Ù…Ø¹Ù„Ø´ ÙŠØ§ Ø²ÙˆØ²ØŒ Ø­ØµÙ„Øª Ù…Ø´ÙƒÙ„Ø© ÙˆØ£Ù†Ø§ Ø¨Ø­Ø§ÙˆÙ„ Ø£Ø¬Ø§ÙˆØ¨. Ù…Ù…ÙƒÙ† ØªØ­Ø§ÙˆÙ„ ØªØ§Ù†ÙŠØŸ"

# --- Unified Persona and Style Guide ---
PERSONA_GUIDE = """
## ROLE AND PERSONA
You are "Zoz the Scientist," an AI Chemistry Tutor for Egyptian Thanaweya Amma students. You are a friendly, empathetic teaching assistant who thinks through problems WITH the student.

VERYYYY IMPORTANNTTTTTT
- **ALWAYS** use English terms like: 'compound', 'ions', 'electrons', 'oxidation state', 'charge', 'energy levels', 'orbitals', 'configuration', 'element', 'series', 'atomic number' , etc .
- NEVER NEVER NEVER USE ANY CHEMICAL EXPRESSION IN ARABIC LANGUAGE like : "Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø°Ø±ÙŠ" , "Ø¹Ù†ØµØ± Ø§Ù†ØªÙ‚Ø§Ù„ÙŠ" , "Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¯Ø±ÙˆÙŠ"

**ALWAYS BEGIN WITH "Ø¯Ø§ Ø³Ø¤Ø§Ù„ Ø¬Ù…ÙŠÙ„ ØªØ¹Ø§Ù„Ù‰ Ù†ÙÙƒØ± ÙÙŠÙ‡ Ø³ÙˆØ§"

## BEHAVIORAL STYLE (Adapt to the situation)
Based on the student's question, adopt ONE of the following conversational styles. Do not mix them unnaturally in a single response
- **The "Let's Think Together" Style:** Use for multi-step problems. (e.g., "ØªØ¹Ø§Ù„Ù‰ Ù†ÙÙ‡Ù… Ø§ÙŠÙ‡ ÙÙƒØ±Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§ØµÙ„Ø§ ÙŠØ§ Ø²ÙˆØ² Ù…Ø¹ Ø¨Ø¹Ø¶...")
- **The "Socratic / Guiding" Style:** Use when the problem is straightforward to guide the student. (e.g., "Ø§Ù†Ø§ ÙƒØ¯Ø© Ø³Ø§Ø¹Ø¯ØªÙƒ Ø´ÙˆÙŠØ© ØªÙ„Ù‚Ø· Ø§Ù„Ø³Ø¤Ø§Ù„ ØŒ ØªÙ‚Ø¯Ø± Ø§Ù†Øª Ø¨Ù‚Ù‰ ØªÙ‚ÙˆÙ„ÙŠÙ„ÙŠ Ø§Ù„Ø§Ø¬Ø§Ø¨Ø© Ù‡ØªØ¨Ù‚Ù‰ Ø§ÙŠÙ‡ â¤ØŸ")
- **The "Empathetic / Reassuring" Style:** Use when a question is tricky or contains a trap. (e.g., "Ù‡Ùˆ ÙØ¹Ù„Ø§ Ù…Ø¹Ø§ÙƒÙŠ Ø­Ù‚... Ø¨Ø³ Ù‡Ùˆ Ø³Ø§Ø¹Ø§Øª Ù‚Ù„ÙŠÙ„Ø© Ø¨ÙŠØ­Ø´Ø± Ø§Ù„Zn ÙˆØ³Ø· Ø§Ù„Ø§Ø®ØªÙŠØ§Ø±Ø§Øª... Ù…ØªØ®Ø§ÙÙŠØ´ ÙŠØ¹Ù†ÙŠ...")
- **The "Conceptual Explanation" Style:** Use for explaining core concepts. (e.g., "Ø§Ù„Element X Ø²ÙŠ Ù…Ø§ ÙˆØ§Ø¶Ø­ Ù‡Ùˆ ÙŠÙ‚Ø¯Ø± ÙŠØ¹Ù…Ù„ lose Ù„ five electrons Ø¨Ø³ Ø·Ø¨ Ø§Ø­Ù†Ø§ Ø¹Ø±ÙÙ†Ø§ Ù…Ù†ÙŠÙ†ØŸ...")

### EXTEREMLY IMPORTANNTTTTTT --> MAKE THE ANSWER WELL ORGANIZED AND DO NOT WRITE ENGLISH AND ARABIC WORDS IN THE SAME LINE BECUASE IT IS REFLECTED AND THIS SO ANNOYINGG âŒâŒâŒ 

## ğŸš¨ CRITICAL FORMATTING RULES ğŸš¨

### THIS IS FOR TELEGRAM - NO LaTeX SUPPORT!

âŒ ABSOLUTELY FORBIDDEN:
- Dollar signs: $...$
- LaTeX subscripts: K_2Cr_2O_7
- LaTeX superscripts: Ni^{+4}
- LaTeX brackets: $[Ar] 3d^8$
- Asterisks: *, **, * item

âœ… REQUIRED FORMAT:
- Plain text: Kâ‚‚Crâ‚‚Oâ‚‡ (not $K_2Cr_2O_7$)
- Plain text: Ni+â´ (not $Ni^{+4}$)
- Plain text: [Ar] 3dâ¶ (not $[Ar] 3d^6$)
- Plain text: 4sÂ² 3dâ¶ (not $4s^2 3d^6$)
- Plain text: Fe+Â³ (not $Fe^{+3}$)
- Plain text: Zn (not $_{30}Zn$)

### CORRECT EXAMPLE:
```
Ø¯Ø§ Ø³Ø¤Ø§Ù„ Ø¬Ù…ÙŠÙ„ ØªØ¹Ø§Ù„Ù‰ Ù†ÙÙƒØ± ÙÙŠÙ‡ Ø³ÙˆØ§

Ø·ÙŠØ¨ ØªØ¹Ø§Ù„Ù‰ Ù†Ø´ÙˆÙ:

1. Ø§Ù„ element X Ø¯Ù‡ Ù‡Ùˆ Iron
Ø§Ù„ electronic configuration Ø¨ØªØ§Ø¹Ù‡ [Ar] 4sÂ² 3dâ¶

2. Ø§Ù„ element Y Ø¯Ù‡ Ù‡Ùˆ Cobalt
Ø§Ù„ configuration Ø¨ØªØ§Ø¹Ù‡ [Ar] 4sÂ² 3d7

3. Ù„Ù…Ø§ Ù†Ø´ÙˆÙ Ø§Ù„ oxidation states
Ø§Ù„ Iron ÙÙŠ XO2 Ù‡ÙŠØ¨Ù‚Ù‰ Fe+â´
Ø§Ù„ configuration Ø¨ØªØ§Ø¹Ù‡ Ù‡ÙŠØ¨Ù‚Ù‰ [Ar] 3dâ´
```

### WRONG EXAMPLE (NEVER DO THIS):
```
* X Ù‡Ùˆ Iron ($Fe$), configuration $[Ar] 4s^2 3d^6$
* Y Ù‡Ùˆ Cobalt ($Co$), configuration $[Ar] 4s^2 3d^7$
```

### LINE BREAKS:
- Use line breaks, and spcae between sections
"""


class ScientificCore:
    """
    Enhanced Scientific Core with CORRECTED Caching Logic.
    
    NEW BEHAVIOR:
    - DO NOT cache answers immediately
    - Only check cache for APPROVED answers
    - Cache only happens when admin approves/corrects in dashboard
    - All data persists in PostgreSQL database
    """
    
    def __init__(self):
        """Initialize the scientific core with all necessary components."""
        self.model = self._initialize_model()
        self.vector_store = self._initialize_vector_store()
        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
        
        if not self.vector_store or not self.model:
            logger.error("Scientific core is not fully available.")
        else:
            logger.info("Scientific core initialized - Cache-on-approval mode enabled")

    def _initialize_model(self):
        """Initialize the Gemini 2.0 Flash model."""
        try:
            genai.configure(api_key=SCIENTIFIC_GENAI_API_KEY)
            model = genai.GenerativeModel('gemini-2.0-flash-exp')
            logger.info("Gemini 2.0 Flash model initialized successfully")
            return model
        except Exception as e:
            logger.error(f"Error initializing scientific Gemini model: {e}", exc_info=True)
            return None

    def _initialize_vector_store(self):
        """Initialize the FAISS vector store for scientific knowledge."""
        try:
            vector_store_path = os.path.join(VECTOR_STORE_DIR, "scientific_index")
            if not os.path.exists(vector_store_path):
                logger.error(f"Scientific vector index not found at {vector_store_path}")
                return None
            
            embeddings = GoogleGenerativeAIEmbeddings(
                model="models/text-embedding-004", 
                google_api_key=SCIENTIFIC_GENAI_API_KEY
            )
            vector_store = FAISS.load_local(
                vector_store_path, 
                embeddings, 
                allow_dangerous_deserialization=True
            )
            logger.info("Scientific vector store loaded successfully")
            return vector_store
        except Exception as e:
            logger.error(f"Error initializing scientific vector store: {e}", exc_info=True)
            return None

    def _compute_image_hash(self, image_path: str) -> str:
        """Compute SHA-256 hash of an image file for exact duplicate detection."""
        try:
            with open(image_path, 'rb') as f:
                file_hash = hashlib.sha256(f.read()).hexdigest()
            logger.debug(f"Computed image hash: {file_hash[:16]}...")
            return file_hash
        except Exception as e:
            logger.error(f"Error computing image hash: {e}")
            return None

    def _retrieve_and_rerank(self, question: str, top_k=8):
        """Retrieve relevant documents and rerank them using cross-encoder."""
        retrieved_docs = self.vector_store.similarity_search(question, k=20)
        
        if not retrieved_docs:
            logger.warning("No documents retrieved from vector store")
            return []
        
        logger.info(f"Retrieved {len(retrieved_docs)} documents from vector store")
        
        # Rerank with cross-encoder
        pairs = [[question, doc.page_content] for doc in retrieved_docs]
        scores = self.reranker.predict(pairs)
        
        doc_scores = list(zip(retrieved_docs, scores))
        doc_scores.sort(key=lambda x: x[1], reverse=True)
        
        top_docs = [doc for doc, score in doc_scores[:top_k]]
        logger.info(f"Reranked to top {len(top_docs)} documents")
        
        return top_docs

    def _relevance_gate(self, question: str, context_docs: list) -> bool:
        """Relevance gate to determine if context is sufficient to answer the question."""
        if not context_docs:
            logger.warning("Relevance gate: No context documents provided")
            return False

        context = "\n\n---\n\n".join([doc.page_content for doc in context_docs])

        prompt = f"""
        You are an expert gatekeeper. Your task is to determine if the provided context has enough information to answer the user's question.

        **User Question:**
        "{question}"

        **Provided Context:**
        ---
        {context}
        ---

        **Your Analysis:**
        1.  Read the user's question to understand what specific information is needed.
        2.  Carefully read the provided context to see if that information is present, even if it requires a calculation based on the context's data (like finding oxidation states).
        3.  Provide your reasoning and a final decision in a JSON format.

        **JSON Output:**
        {{
          "reasoning": "A brief explanation of why the context is or is not sufficient.",
          "decision": "YES" or "NO"
        }}
        """
        
        try:
            response = self.model.generate_content(prompt)
            
            if not response or not response.text:
                logger.warning("Relevance gate received empty response from Gemini")
                return False
            
            clean_json_str = response.text.strip().replace("```json", "").replace("```", "")
            result = json.loads(clean_json_str)

            logger.info(f"Relevance Gate Reasoning: {result.get('reasoning')}")
            decision = result.get('decision', 'NO').upper() == 'YES'
            
            if decision:
                logger.info("âœ“ Relevance gate PASSED - Context is sufficient")
            else:
                logger.warning("âœ— Relevance gate FAILED - Context is insufficient")
            
            return decision

        except (json.JSONDecodeError, KeyError) as e:
            logger.error(f"Relevance Gate could not parse JSON response: {e}")
            return False
        except Exception as e:
            logger.error(f"Relevance Gate encountered an unknown error: {e}")
            return False

    async def classify_followup(self, message: str) -> str:
        """Classify the intent of a follow-up message."""
        prompt = f"""
        You are an expert intent classifier. Analyze the user's message and determine its intent.
        The user has just received an answer to a scientific question.

        Possible intents are:
        - "thanks": The user is expressing gratitude or confirmation of understanding (e.g., "thanks", "shokran", "ØªÙ…Ø§Ù… ÙÙ‡Ù…Øª", "got it").
        - "re_explain": The user is expressing confusion or asking for more clarification (e.g., "I don't understand", "Ù…Ø´ ÙØ§Ù‡Ù…", "explain again", "ÙˆØ¶Ø­ ØªØ§Ù†ÙŠ").
        - "new_question": The user is asking a completely new and different question.

        User Message: "{message}"

        Provide your answer in JSON format with a single key "intent".
        """
        try:
            response = await self.model.generate_content_async(prompt)
            
            if not response or not response.text:
                logger.warning("Follow-up classification received empty response")
                return "new_question"
            
            clean_json_str = response.text.strip().replace("```json", "").replace("```", "")
            result = json.loads(clean_json_str)
            intent = result.get("intent", "new_question")
            logger.info(f"Follow-up intent classified as: {intent}")
            return intent
        except Exception as e:
            logger.error(f"Could not classify follow-up intent: {e}")
            return "new_question"

    async def re_explain_answer(self, original_question: str, previous_answer: str) -> str:
        """Generate a new explanation while maintaining the "Zoz the Scientist" persona."""
        prompt = f"""
        {PERSONA_GUIDE}


        ## TASK:
        Re-explain the concept using a different approach.
        - Start with: "ØªØ¹Ø§Ù„Ù‰ Ù†ÙÙƒØ± ÙÙŠ Ø§Ù„Ø³Ø¤Ø§Ù„ ØªØ§Ù†ÙŠ"
        - Keep same persona
        - Break down steps more clearly
        - NO LaTeX, NO asterisks
        - Use plain text only
        
         - option a) [text]
           [explanation]
       
       - option b) [text]
           [explanation]

           
### EXTEREMLY IMPORTANNTTTTTT --> MAKE THE ANSWER WELL ORGANIZED AND DO NOT WRITE ENGLISH AND ARABIC WORDS IN THE SAME LINE BECUASE IT IS REFLECTED AND THIS SO ANNOYINGG âŒâŒâŒ 

## ğŸš¨ CRITICAL FORMATTING RULES ğŸš¨

### THIS IS FOR TELEGRAM - NO LaTeX SUPPORT!

âŒ ABSOLUTELY FORBIDDEN:
- Dollar signs: $...$
- LaTeX subscripts: K_2Cr_2O_7
- LaTeX superscripts: Ni^{+4}
- LaTeX brackets: $[Ar] 3d^8$
- Asterisks: *, **, * item

âœ… REQUIRED FORMAT:
- Plain text: Kâ‚‚Crâ‚‚Oâ‚‡ (not $K_2Cr_2O_7$)
- Plain text: Ni+â´ (not $Ni^{+4}$)
- Plain text: [Ar] 3dâ¶ (not $[Ar] 3d^6$)
- Plain text: 4sÂ² 3dâ¶ (not $4s^2 3d^6$)
- Plain text: Fe+Â³ (not $Fe^{+3}$)
- Plain text: Zn (not $_{30}Zn$)

        ## CONTEXT:
        **Original Question:** "{original_question}"
        **Your Previous Answer:** "{previous_answer}"

        ## Your New Explanation (PLAIN TEXT ONLY - NO LaTeX, NO asterisks):
        """
        
        try:
            safety_settings = {
                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
            }
            
            response = await self.model.generate_content_async(
                prompt,
                safety_settings=safety_settings
            )
            
            if not response or not response.text:
                return "Ø­ØµÙ„ Ù…Ø´ÙƒÙ„Ø© ÙˆØ£Ù†Ø§ Ø¨Ø­Ø§ÙˆÙ„ Ø£Ø´Ø±Ø­ ØªØ§Ù†ÙŠ. Ù…Ù…ÙƒÙ† ØªØ¨Ø¹Øª Ø§Ù„Ø³Ø¤Ø§Ù„ Ù…Ø±Ø© ØªØ§Ù†ÙŠØ©ØŸ"
            
            logger.info("Re-explanation generated successfully")
            return response.text
        except Exception as e:
            logger.error(f"Error during re-explanation: {e}")
            return "Ø­ØµÙ„ Ù…Ø´ÙƒÙ„Ø© ÙˆØ£Ù†Ø§ Ø¨Ø­Ø§ÙˆÙ„ Ø£Ø´Ø±Ø­ ØªØ§Ù†ÙŠ. Ù…Ù…ÙƒÙ† ØªØ¨Ø¹Øª Ø§Ù„Ø³Ø¤Ø§Ù„ Ù…Ø±Ø© ØªØ§Ù†ÙŠØ©ØŸ"

    def _extract_json(self, text: str) -> str:
        """Extract JSON object from text that might contain additional content."""
        import re
        
        text = text.replace("```json", "").replace("```", "")
        
        json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
        matches = re.findall(json_pattern, text, re.DOTALL)
        
        if matches:
            for match in reversed(matches):
                try:
                    json.loads(match)
                    return match
                except json.JSONDecodeError:
                    continue
        
        last_brace = text.rfind('{')
        if last_brace != -1:
            potential_json = text[last_brace:]
            try:
                json.loads(potential_json)
                return potential_json
            except json.JSONDecodeError:
                pass
        
        return ""

    async def get_scientific_response_async(
        self, 
        user_question: str = None, 
        image_path: str = None
        
    ) -> tuple:
        
        if not self.vector_store or not self.model:
            logger.error("Scientific core not available")
            return FALLBACK_ERROR, None, None

        transcribed_question = None
        image_hash = None
        cached_question_id = None

        # ========================================
        # STEP 1: IMAGE TRANSCRIPTION (if provided)
        # ========================================
        if image_path:
            try:
                logger.info(f"ğŸ“¸ Processing image at {image_path}")
                
                # CRITICAL FIX: Compute hash from the ACTUAL file that was just saved
                image_hash = self._compute_image_hash(image_path)
                if image_hash:
                    logger.info(f"ğŸ”’ Computed image hash: {image_hash[:16]}...")
                else:
                    logger.warning("âš ï¸ Failed to compute image hash")
                
                img = PIL.Image.open(image_path)
                
                safety_settings = {
                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
                }
                
                # --- NEW RETRY LOGIC for 429 ResourceExhausted ---
                max_retries = 3
                base_delay_seconds = 2
                transcribed_question = None

                for attempt in range(max_retries):
                    try:
                        transcription_response = self.model.generate_content(
                            [
                                "Please transcribe the following chemistry question from the image. "
                                "Extract all text, including choices. Be consistent and precise and if is there side notes written by a student please neglect it (do not transcribe the side notes)", 
                                img
                            ],
                            safety_settings=safety_settings
                        )
                        
                        if not transcription_response or not transcription_response.text:
                            logger.error("Image transcription returned empty response")
                            # Don't retry on empty, just fail
                            return "Ù…Ø¹Ù„Ø´ ÙŠØ§ Ø²ÙˆØ²ØŒ Ù…Ù‚Ø¯Ø±ØªØ´ Ø£Ù‚Ø±Ø£ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù„ÙŠ Ø¨Ø¹ØªÙ‡Ø§. Ù…Ù…ÙƒÙ† ØªØªØ£ÙƒØ¯ Ø¥Ù†Ù‡Ø§ ÙˆØ§Ø¶Ø­Ø© ÙˆØªØ¨Ø¹ØªÙ‡Ø§ ØªØ§Ù†ÙŠØŸ", None, None
                        
                        transcribed_question = transcription_response.text
                        logger.info(f"âœ“ Transcribed: '{transcribed_question[:100]}...'")
                        user_question = transcribed_question
                        break  # --- SUCCESS, exit retry loop ---

                    except ResourceExhausted as e:
                        if attempt < max_retries - 1:
                            # Exponential backoff: 2s, 4s, 8s
                            wait_time = base_delay_seconds * (2 ** attempt) 
                            logger.warning(f"Rate limit hit (429). Retrying in {wait_time} seconds... (Attempt {attempt + 1}/{max_retries})")
                            time.sleep(wait_time) # Use time.sleep since generate_content is synchronous
                        else:
                            logger.error(f"âœ— Failed to transcribe image after {max_retries} attempts due to rate limiting.")
                            raise e # Re-raise the final exception to be caught by the outer block
                # --- END RETRY LOGIC ---

                if not transcribed_question:
                    # This should only happen if the loop fails without an exception
                    raise Exception("Transcription failed after retries.")
                
            except Exception as e:
                logger.error(f"âœ— Error processing image: {e}", exc_info=True)
                return "Ù…Ø¹Ù„Ø´ ÙŠØ§ Ø²ÙˆØ²ØŒ Ù…Ù‚Ø¯Ø±ØªØ´ Ø£Ù‚Ø±Ø£ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù„ÙŠ Ø¨Ø¹ØªÙ‡Ø§. Ù…Ù…ÙƒÙ† ØªØªØ£ÙƒØ¯ Ø¥Ù†Ù‡Ø§ ÙˆØ§Ø¶Ø­Ø© ÙˆØªØ¨Ø¹ØªÙ‡Ø§ ØªØ§Ù†ÙŠØŸ", None, None

        if not user_question:
            logger.error("No question provided")
            return FALLBACK_ERROR, None, None

        # ========================================
        # STEP 2: CHECK CACHE - FIXED LOGIC
        # ========================================
        logger.info("ğŸ” Checking cache for APPROVED/CORRECTED answers...")
        
        # PRIORITY 1: Check by image hash first (most reliable)
        if image_hash:
            logger.info(f"ğŸ” Searching cache by image hash: {image_hash[:16]}...")
            similar_question = await deduplicator.find_similar_question(
                question_text=user_question,
                image_hash=image_hash
            )
            
            if similar_question and similar_question.get('is_corrected'):
                logger.info(
                    f"âœ“ CACHE HIT (IMAGE HASH)! Returning APPROVED answer "
                    f"(ID: {similar_question['id']}, used {similar_question['times_used']} times)"
                )
                cached_response = similar_question['answer_text']
                cached_question_id = similar_question['id']
                return cached_response, cached_question_id, user_question
            elif similar_question:
                logger.info(
                    f"â„¹ï¸ Image hash match found (ID: {similar_question['id']}) "
                    f"BUT NOT YET APPROVED - processing normally"
                )
        else:
            # PRIORITY 2: Text-based search (for text-only questions)
            logger.info("ğŸ” No image - searching cache by text similarity...")
            similar_question = await deduplicator.find_similar_question(
                question_text=user_question,
                image_hash=None
            )
            
            if similar_question and similar_question.get('is_corrected'):
                logger.info(
                    f"âœ“ CACHE HIT (TEXT)! Returning APPROVED answer "
                    f"(ID: {similar_question['id']}, used {similar_question['times_used']} times)"
                )
                cached_response = similar_question['answer_text']
                cached_question_id = similar_question['id']
                return cached_response, cached_question_id, user_question
            elif similar_question:
                logger.info(
                    f"â„¹ï¸ Similar question found (ID: {similar_question['id']}) "
                    f"BUT NOT YET APPROVED - processing normally"
                )

        # ========================================
        # STEP 3: NO APPROVED CACHE - PROCESS WITH RAG
        # ========================================
        logger.info("âœ— No approved cache hit - processing with RAG pipeline")
        logger.info(f"ğŸ“š Starting retrieval for: '{user_question[:50]}...'")
        
        reranked_docs = self._retrieve_and_rerank(user_question)

        if not self._relevance_gate(user_question, reranked_docs):
            logger.warning("âœ— Relevance gate failed")
            # Use transcribed_question if available, fall back to user_question
            return FALLBACK_NO_INFO, None, transcribed_question or user_question

        logger.info("âœ“ Relevance gate passed - generating answer")
        
        context_with_ids = "\n\n".join([
            f"Source ID: {doc.metadata['source_id']}\nContent: {doc.page_content}" 
            for doc in reranked_docs
        ])

        # ========================================
        # STEP 4: GENERATE ANSWER
        # ========================================
        generation_prompt = f"""
        {PERSONA_GUIDE}

        
        ## CRITICAL FORMATTING RULES FOR "final_answer":
        1. Start with: "Ø¯Ø§ Ø³Ø¤Ø§Ù„ Ø¬Ù…ÙŠÙ„ ØªØ¹Ø§Ù„Ù‰ Ù†ÙÙƒØ± ÙÙŠÙ‡ Ø³ÙˆØ§" on its own line
        2. Add a blank line after the opening
        3. Use proper paragraph breaks (\\n\\n) between major sections
        4. When listing options, use this EXACT format with proper spacing:
       Ø·ÙŠØ¨ØŒ ØªØ¹Ø§Ù„ Ù†Ø´ÙˆÙ Ø§Ù„ options Ø§Ù„Ù„ÙŠ Ø¹Ù†Ø¯Ù†Ø§:
       
       - option a) [text]
           [explanation]
       
       - option b) [text]
           [explanation]



### EXTEREMLY IMPORTANNTTTTTT --> MAKE THE ANSWER WELL ORGANIZED AND DO NOT WRITE ENGLISH AND ARABIC WORDS IN THE SAME LINE BECUASE IT IS REFLECTED AND THIS SO ANNOYINGG âŒâŒâŒ 


        ## ğŸš¨ğŸš¨ğŸš¨ TELEGRAM FORMAT - NO LaTeX ğŸš¨ğŸš¨ğŸš¨

        BEFORE WRITING ANYTHING:
        1. âŒ NO dollar signs ($) anywhere
        2. âŒ NO subscripts with underscores (K_2)
        3. âŒ NO superscripts with carets (^2)
        4. âŒ NO asterisks (*) for bullets or bold
        5. âŒ NO LaTeX brackets: $[Ar] 3d^8$
        6. âœ… ONLY plain text: Kâ‚‚Crâ‚‚Oâ‚‡, Ni+â´, [Ar] 3dâ¸, Fe+Â³, 4s2 3dâ¸

        ## CORRECT JSON EXAMPLE:
        {{
          "final_answer": "Ø¯Ø§ Ø³Ø¤Ø§Ù„ Ø¬Ù…ÙŠÙ„ ØªØ¹Ø§Ù„Ù‰ Ù†ÙÙƒØ± ÙÙŠÙ‡ Ø³ÙˆØ§\\n\\nØ·ÙŠØ¨ ØªØ¹Ø§Ù„Ù‰ Ù†Ø´ÙˆÙ:\\n\\n1. Ø§Ù„ element X Ø¯Ù‡\\nØ§Ù„ electronic configuration Ø¨ØªØ§Ø¹Ù‡ [Ar] 4s2 3d6\\nÙ„Ù…Ø§ ÙŠÙÙ‚Ø¯ 4 electrons Ù‡ÙŠØ¨Ù‚Ù‰ [Ar] 3d4\\n\\n2. Ø§Ù„ element Y Ø¯Ù‡\\nØ§Ù„ configuration Ø¨ØªØ§Ø¹Ù‡ [Ar] 4s2 3d7\\nÙ„Ù…Ø§ ÙŠÙÙ‚Ø¯ 3 electrons Ù‡ÙŠØ¨Ù‚Ù‰ [Ar] 3d6\\n\\nÙŠØ¨Ù‚Ù‰ Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ØµØ­ Ù‡Ùˆ Ø±Ù‚Ù… 2",
          "sources": ["Chapter1-chunk5"]
        }}

        ## WRONG JSON EXAMPLE (NEVER DO THIS):
        {{
          "final_answer": "* X Ù‡Ùˆ $Fe$, configuration $[Ar] 4s^2 3d^6$\\n* Y Ù‡Ùˆ $Co$"
        }}

        ## TASK:
        Answer the student's question using ONLY the provided Curriculum Content.
        - Output MUST be JSON with "final_answer" and "sources"
        - Use ONLY plain text in final_answer (NO LaTeX, NO asterisks)
        - Start with: "Ø¯Ø§ Ø³Ø¤Ø§Ù„ Ø¬Ù…ÙŠÙ„ ØªØ¹Ø§Ù„Ù‰ Ù†ÙÙƒØ± ÙÙŠÙ‡ Ø³ÙˆØ§"
        - Use line breaks, and spcae between sections
        - NEVER mention Source IDs in the final answer
        - Write chemical formulas in plain text: Fe+Â³, [Ar] 3dâ¸, Kâ‚‚Crâ‚‚Oâ‚‡

        
        ---
        ## Curriculum Content
        {context_with_ids}
        ---

        ## Student's Question
        {user_question}

        ## Your JSON Output (PLAIN TEXT ONLY - NO $, NO LaTeX, NO asterisks):
        """

        try:
            safety_settings = {
                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
            }

            response = self.model.generate_content(
                generation_prompt,
                safety_settings=safety_settings
            )

            if not response or not response.text:
                logger.error("Generation response empty or blocked")
                return FALLBACK_ERROR, None, transcribed_question or user_question

            response_text = response.text.strip()
            
            if not response_text:
                logger.error("Response text empty")
                return FALLBACK_ERROR, None, transcribed_question or user_question
            
            json_str = self._extract_json(response_text)
            
            if not json_str:
                logger.error("Could not extract JSON from response")
                return FALLBACK_ERROR, None, transcribed_question or user_question
            
            result = json.loads(json_str)
            final_answer = result.get("final_answer", "")
            sources = result.get("sources", [])
            
            if not final_answer:
                logger.warning("Final answer empty")
                return FALLBACK_NO_INFO, None, transcribed_question or user_question
            
            logger.info(f"âœ“ Answer generated ({len(final_answer)} chars)")
            
            # ========================================
            # Wait for admin approval/correction first
            # ========================================
            logger.info("â„¹ï¸ Answer generated but NOT cached - waiting for admin approval")
            logger.info("ğŸ“ Answer will be saved to interactions table for admin review")
            
            return final_answer, None, transcribed_question or user_question
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error: {e}")
            return FALLBACK_ERROR, None, transcribed_question or user_question
        except Exception as e:
            logger.error(f"Unknown error: {e}", exc_info=True)
            return FALLBACK_ERROR, None, transcribed_question or user_question

    def get_scientific_response(
        self, 
        user_question: str = None, 
        image_path: str = None
    ) -> str:
        """Synchronous wrapper for get_scientific_response_async."""
        try:
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            
            # Update tuple unpacking to account for the third return value
            response, cached_id, transcribed_question = loop.run_until_complete(
                self.get_scientific_response_async(user_question, image_path)
            )
            
            return response
            
        except Exception as e:
            logger.error(f"Error in synchronous wrapper: {e}", exc_info=True)
            return FALLBACK_ERROR


# ========================================
# SINGLETON INSTANCE
# ========================================
scientific_core_instance = ScientificCore()
